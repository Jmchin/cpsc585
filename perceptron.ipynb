{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perceptrons are artificial neurons that are roughly modeled on neurobiology. The perceptron can be conceptualized as a body, which receives some number of weighted inputs. This body performs the weighted summation of these weights and inputs, comparing it to an internal threshold value. If the weighted sum exceeds the threshold, the output state is +1, otherwise, output state is -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dataset\n",
    "import dataset\n",
    "\n",
    "# Our dataset uses chars and not number values: encode the values (bipolar)\n",
    "def encode(data):\n",
    "    return [1 if pixel == '#' else -1\n",
    "               for line in data\n",
    "               for pixel in line]\n",
    "\n",
    "train_set = [encode(d) for d in dataset.TRAINING_DATA]\n",
    "test_set = [encode(d) for d in dataset.TEST_DATA]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our data, we can do some superficial analysis to make sure we are on the right track. Our goal is to implement a perceptron for each letter of the English alphabet, for a total of 26 perceptrons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(train_set): 26\n",
      "len(test_set): 26\n"
     ]
    }
   ],
   "source": [
    "print(f'len(train_set): {len(train_set)}')\n",
    "print(f'len(test_set): {len(test_set)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks good so far. We need to implement 26 perceptrons, and we have 26 items in both the training set and testing sets.\n",
    "\n",
    "Now we can check the length of our input vector for our letter classifying perceptron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(input_vector): 35\n"
     ]
    }
   ],
   "source": [
    "print(f'len(input_vector): {len(train_set[0])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The letters we are using for our training and testing data are structured as 5x7 blocks, so this seems to check out as well.\n",
    "\n",
    "Now that we know the length of our input vector, we can initialize our weights vector.\n",
    "\n",
    "NOTE: In the perceptron model, there is an additional bias input, which is always a weighted +1. We will need to account for this by adding an extra weight to our weight vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_vector = [1] + train_set[0]         # add bias to front of input vector\n",
    "input_vector = np.asarray(input_vector)   # lets read everything in as numpy arrays now\n",
    "\n",
    "np.random.seed(1)  # for testing purposes\n",
    "weight_vector = np.random.uniform(-1, 1, len(input_vector))  # initialize random weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have our two vectors describing the perceptron network, we can start modeling the rest. Right now we have inputs and their associated weights. With these, we can compute a weighted sum of all the inputs into the perceptron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.16595599  0.44064899 -0.99977125 -0.39533485 -0.70648822 -0.81532281\n",
      " -0.62747958 -0.30887855 -0.20646505  0.07763347 -0.16161097  0.370439\n",
      " -0.5910955   0.75623487 -0.94522481  0.34093502 -0.1653904   0.11737966\n",
      " -0.71922612 -0.60379702  0.60148914  0.93652315 -0.37315164  0.38464523\n",
      "  0.7527783   0.78921333 -0.82991158 -0.92189043 -0.66033916  0.75628501\n",
      " -0.80330633 -0.15778475  0.91577906  0.06633057  0.38375423 -0.36896874]\n",
      "--------------------------------------------------------------------------------\n",
      "[ 1 -1  1  1  1 -1  1 -1 -1 -1  1  1 -1 -1 -1  1  1 -1 -1 -1  1  1  1  1\n",
      "  1  1  1 -1 -1 -1  1  1 -1 -1 -1  1]\n",
      "--------------------------------------------------------------------------------\n",
      "0.6790624853886102\n"
     ]
    }
   ],
   "source": [
    "weighted_sum = np.dot(weight_vector, input_vector)\n",
    "\n",
    "print(weight_vector.transpose())\n",
    "print('-'*80)\n",
    "print(input_vector)\n",
    "print('-'*80)\n",
    "print(weighted_sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next part we need to add into our perceptron model is the activation function, i.e a threshold. If the weighted sum of all inputs to the network exceeds some threshold value ouput +1, else output -1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network predicts 1 given (0.6790624853886102) weighted sum.\n"
     ]
    }
   ],
   "source": [
    "threshold = 0\n",
    "prediction = 1 if weighted_sum > threshold else -1\n",
    "print(f'Network predicts {prediction} given ({weighted_sum}) weighted sum.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We almost have all of the building blocks we need to implement a perceptron. Now we need to figure out how to make the perceptron \"learn\". That is, we need to compare our prediction to the target value, and update our weights accordingly.\n",
    "\n",
    "To do this, first we'll need to define the labels for correctly classifying each letter. Right now let's just quickly build it manually. We can automate this process, for this specific problem, later when we're ready to turn this into an implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1 -1  1  1  1 -1  1 -1 -1 -1  1  1 -1 -1 -1  1  1 -1 -1 -1  1  1  1  1\n",
      "  1  1  1 -1 -1 -1  1  1 -1 -1 -1  1]\n",
      "-0.165955990594852\n",
      "[-0.16595599  0.44064899 -0.99977125 -0.39533485 -0.70648822 -0.81532281\n",
      " -0.62747958 -0.30887855 -0.20646505  0.07763347 -0.16161097  0.370439\n",
      " -0.5910955   0.75623487 -0.94522481  0.34093502 -0.1653904   0.11737966\n",
      " -0.71922612 -0.60379702  0.60148914  0.93652315 -0.37315164  0.38464523\n",
      "  0.7527783   0.78921333 -0.82991158 -0.92189043 -0.66033916  0.75628501\n",
      " -0.80330633 -0.15778475  0.91577906  0.06633057  0.38375423 -0.36896874]\n",
      "36\n",
      "36\n",
      "dot prod: 0.6790624853886102\n",
      "dot + weight[0]: 0.67906248538861\n"
     ]
    }
   ],
   "source": [
    "# Setup labels vector for recognizing 'A'\n",
    "labels = [-1] * len(train_set)\n",
    "labels[0] = 1\n",
    "\n",
    "# Add +1 bias to beginning of all input vectors in training set\n",
    "inputs = [[1] + train_set[i] for i in range(len(train_set))]\n",
    "\n",
    "# randomize weights\n",
    "np.random.seed(1)\n",
    "weights = np.random.uniform(-1, 1, (len(inputs[0])))\n",
    "\n",
    "# load in testing set\n",
    "test_inputs = [[1] + test_set[i] for i in range(len(test_set))]\n",
    "\n",
    "# convert to np.arrays\n",
    "labels = np.asarray(labels)\n",
    "inputs = np.asarray(inputs)\n",
    "weights = np.asarray(weights)\n",
    "test_inputs = np.asarray(test_inputs)\n",
    "\n",
    "# Perceptron classifies in {0, 1}\n",
    "def step(x, threshold=0):\n",
    "    return 1 if x > threshold else 0\n",
    "\n",
    "# calculate output state for each input vector with weights, step output, compare to expected label\n",
    "#X = zip(inputs, labels)\n",
    "#for i, (a, b) in enumerate(X):\n",
    "#    output = np.dot(weights, a)\n",
    "#    prediction = step(output)\n",
    "#    print(f'output (x_{i}): {output}, {prediction}, {\"hit\" if prediction == b else \"miss\"}')\n",
    "    \n",
    "def fit(weights, inputs, labels, activation=step, learning_rate=0.01, epochs=10):\n",
    "    learning = True\n",
    "    iters = 0\n",
    "    misclasses = []\n",
    "    \n",
    "    # until all inputs classified properly, or enough epochs have passed\n",
    "    while learning and iters < epochs:\n",
    "        # predict output, compare to expected label, update weights\n",
    "        misclass_count = 0\n",
    "        learning = False\n",
    "        \n",
    "        for i, (ins, label) in enumerate(zip(inputs, labels)):\n",
    "            summation = np.dot(weights[1:], ins[1:]) + weights[0]  # weighted sum\n",
    "            prediction = activation(summation)      # classify output\n",
    "            \n",
    "            def update_weight(w, lr, labl, pred, inp):\n",
    "                error = inp\n",
    "                return w + (lr * )\n",
    "            \n",
    "            weights = []\n",
    "            for i in range(len(weights)):\n",
    "                weights[i] += learning_rate * (label - prediction) * ins[i]\n",
    "                \n",
    "            #if prediction != label:\n",
    "             #   learning = True\n",
    "              #  misclass_count += 1\n",
    "                \n",
    "                # update weights\n",
    "               # delta_w = learning_rate * (label - prediction) * ins\n",
    "                #weights = np.add(weights, delta_w)\n",
    "                \n",
    "            #print(f'output (x_{i}): {output}, {prediction}, {\"hit\" if prediction == label else \"miss\"}')\n",
    "                \n",
    "        #print(f'epoch {iters} : {misclass_count} misses\\n')\n",
    "        misclasses.append(misclass_count)\n",
    "        iters = iters + 1\n",
    "    return weights, misclasses\n",
    "\n",
    "def plot(misclasses, title):\n",
    "    print(f'length: {len(misclasses)} {misclasses}')\n",
    "    fig, ax = plt.subplots()\n",
    "    \n",
    "    ax.set(xlabel='Number of Iterations', ylabel='Number of Misclassifications',\n",
    "       title=f'Perceptron Training: {title}')\n",
    "    \n",
    "    xaxis = np.arange(0, len(misclasses), 1)\n",
    "    yaxis = misclasses\n",
    "    ax.plot(xaxis, yaxis)\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "char_map = [ch for ch in \"abcdefghijklmnopqrstuvwxyz\"]\n",
    "all_labels = [np.roll(labels, i) for i in range(len(labels))]  # generate labels for all letters we want to classify\n",
    "\n",
    "\n",
    "print(inputs[0])\n",
    "print(weights[0])\n",
    "print(weights)\n",
    "print(len(inputs[0]))\n",
    "print(len(weights))\n",
    "print(f'dot prod: {np.dot(weights, inputs[0])}')\n",
    "print(f'dot + weight[0]: {np.dot(weights[1:], inputs[0][1:]) + weights[0]}')\n",
    "#trained_classes, misclasses = fit(weights, inputs, labels, step)\n",
    "#plot(misclasses, char_map[0])\n",
    "\n",
    "#print(all_labels)\n",
    "#for i, labels in enumerate(all_labels):\n",
    "    # train the perceptron\n",
    "    #trained_weights, misclasses = fit(weights, inputs, labels, step)\n",
    "    #plot(misclasses, char_map[i])\n",
    "    \n",
    "    # test the perceptron\n",
    "    #output = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.16595599  0.44064899 -0.99977125 -0.39533485 -0.70648822 -0.81532281\n",
      " -0.62747958 -0.30887855 -0.20646505  0.07763347 -0.16161097  0.370439\n",
      " -0.5910955   0.75623487 -0.94522481  0.34093502 -0.1653904   0.11737966\n",
      " -0.71922612 -0.60379702  0.60148914  0.93652315 -0.37315164  0.38464523\n",
      "  0.7527783   0.78921333 -0.82991158 -0.92189043 -0.66033916  0.75628501\n",
      " -0.80330633 -0.15778475  0.91577906  0.06633057  0.38375423 -0.36896874]\n",
      "prediction: 1\n",
      "old weights: [-0.16595599  0.44064899 -0.99977125 -0.39533485 -0.70648822 -0.81532281\n",
      " -0.62747958 -0.30887855 -0.20646505  0.07763347 -0.16161097  0.370439\n",
      " -0.5910955   0.75623487 -0.94522481  0.34093502 -0.1653904   0.11737966\n",
      " -0.71922612 -0.60379702  0.60148914  0.93652315 -0.37315164  0.38464523\n",
      "  0.7527783   0.78921333 -0.82991158 -0.92189043 -0.66033916  0.75628501\n",
      " -0.80330633 -0.15778475  0.91577906  0.06633057  0.38375423 -0.36896874]\n",
      "new weights: [-0.16595599  0.44064899 -0.99977125 -0.39533485 -0.70648822 -0.81532281\n",
      " -0.62747958 -0.30887855 -0.20646505  0.07763347 -0.16161097  0.370439\n",
      " -0.5910955   0.75623487 -0.94522481  0.34093502 -0.1653904   0.11737966\n",
      " -0.71922612 -0.60379702  0.60148914  0.93652315 -0.37315164  0.38464523\n",
      "  0.7527783   0.78921333 -0.82991158 -0.92189043 -0.66033916  0.75628501\n",
      " -0.80330633 -0.15778475  0.91577906  0.06633057  0.38375423 -0.36896874]\n",
      "prediction: 1\n",
      "old weights: [-0.16595599  0.44064899 -0.99977125 -0.39533485 -0.70648822 -0.81532281\n",
      " -0.62747958 -0.30887855 -0.20646505  0.07763347 -0.16161097  0.370439\n",
      " -0.5910955   0.75623487 -0.94522481  0.34093502 -0.1653904   0.11737966\n",
      " -0.71922612 -0.60379702  0.60148914  0.93652315 -0.37315164  0.38464523\n",
      "  0.7527783   0.78921333 -0.82991158 -0.92189043 -0.66033916  0.75628501\n",
      " -0.80330633 -0.15778475  0.91577906  0.06633057  0.38375423 -0.36896874]\n",
      "new weights: [-0.18595599  0.42064899 -1.01977125 -0.41533485 -0.72648822 -0.83532281\n",
      " -0.64747958 -0.32887855 -0.22646505  0.05763347 -0.18161097  0.350439\n",
      " -0.6110955   0.73623487 -0.96522481  0.32093502 -0.1853904   0.09737966\n",
      " -0.73922612 -0.62379702  0.58148914  0.91652315 -0.39315164  0.36464523\n",
      "  0.7327783   0.76921333 -0.84991158 -0.94189043 -0.68033916  0.73628501\n",
      " -0.82330633 -0.17778475  0.89577906  0.04633057  0.36375423 -0.38896874]\n",
      "prediction: 1\n",
      "old weights: [-0.18595599  0.42064899 -1.01977125 -0.41533485 -0.72648822 -0.83532281\n",
      " -0.64747958 -0.32887855 -0.22646505  0.05763347 -0.18161097  0.350439\n",
      " -0.6110955   0.73623487 -0.96522481  0.32093502 -0.1853904   0.09737966\n",
      " -0.73922612 -0.62379702  0.58148914  0.91652315 -0.39315164  0.36464523\n",
      "  0.7327783   0.76921333 -0.84991158 -0.94189043 -0.68033916  0.73628501\n",
      " -0.82330633 -0.17778475  0.89577906  0.04633057  0.36375423 -0.38896874]\n",
      "new weights: [-0.20595599  0.40064899 -1.03977125 -0.43533485 -0.74648822 -0.85532281\n",
      " -0.66747958 -0.34887855 -0.24646505  0.03763347 -0.20161097  0.330439\n",
      " -0.6310955   0.71623487 -0.98522481  0.30093502 -0.2053904   0.07737966\n",
      " -0.75922612 -0.64379702  0.56148914  0.89652315 -0.41315164  0.34464523\n",
      "  0.7127783   0.74921333 -0.86991158 -0.96189043 -0.70033916  0.71628501\n",
      " -0.84330633 -0.19778475  0.87577906  0.02633057  0.34375423 -0.40896874]\n",
      "prediction: 1\n",
      "old weights: [-0.20595599  0.40064899 -1.03977125 -0.43533485 -0.74648822 -0.85532281\n",
      " -0.66747958 -0.34887855 -0.24646505  0.03763347 -0.20161097  0.330439\n",
      " -0.6310955   0.71623487 -0.98522481  0.30093502 -0.2053904   0.07737966\n",
      " -0.75922612 -0.64379702  0.56148914  0.89652315 -0.41315164  0.34464523\n",
      "  0.7127783   0.74921333 -0.86991158 -0.96189043 -0.70033916  0.71628501\n",
      " -0.84330633 -0.19778475  0.87577906  0.02633057  0.34375423 -0.40896874]\n",
      "new weights: [-0.22595599  0.38064899 -1.05977125 -0.45533485 -0.76648822 -0.87532281\n",
      " -0.68747958 -0.36887855 -0.26646505  0.01763347 -0.22161097  0.310439\n",
      " -0.6510955   0.69623487 -1.00522481  0.28093502 -0.2253904   0.05737966\n",
      " -0.77922612 -0.66379702  0.54148914  0.87652315 -0.43315164  0.32464523\n",
      "  0.6927783   0.72921333 -0.88991158 -0.98189043 -0.72033916  0.69628501\n",
      " -0.86330633 -0.21778475  0.85577906  0.00633057  0.32375423 -0.42896874]\n",
      "prediction: 1\n",
      "old weights: [-0.22595599  0.38064899 -1.05977125 -0.45533485 -0.76648822 -0.87532281\n",
      " -0.68747958 -0.36887855 -0.26646505  0.01763347 -0.22161097  0.310439\n",
      " -0.6510955   0.69623487 -1.00522481  0.28093502 -0.2253904   0.05737966\n",
      " -0.77922612 -0.66379702  0.54148914  0.87652315 -0.43315164  0.32464523\n",
      "  0.6927783   0.72921333 -0.88991158 -0.98189043 -0.72033916  0.69628501\n",
      " -0.86330633 -0.21778475  0.85577906  0.00633057  0.32375423 -0.42896874]\n",
      "new weights: [-0.24595599  0.36064899 -1.07977125 -0.47533485 -0.78648822 -0.89532281\n",
      " -0.70747958 -0.38887855 -0.28646505 -0.00236653 -0.24161097  0.290439\n",
      " -0.6710955   0.67623487 -1.02522481  0.26093502 -0.2453904   0.03737966\n",
      " -0.79922612 -0.68379702  0.52148914  0.85652315 -0.45315164  0.30464523\n",
      "  0.6727783   0.70921333 -0.90991158 -1.00189043 -0.74033916  0.67628501\n",
      " -0.88330633 -0.23778475  0.83577906 -0.01366943  0.30375423 -0.44896874]\n",
      "prediction: 1\n",
      "old weights: [-0.24595599  0.36064899 -1.07977125 -0.47533485 -0.78648822 -0.89532281\n",
      " -0.70747958 -0.38887855 -0.28646505 -0.00236653 -0.24161097  0.290439\n",
      " -0.6710955   0.67623487 -1.02522481  0.26093502 -0.2453904   0.03737966\n",
      " -0.79922612 -0.68379702  0.52148914  0.85652315 -0.45315164  0.30464523\n",
      "  0.6727783   0.70921333 -0.90991158 -1.00189043 -0.74033916  0.67628501\n",
      " -0.88330633 -0.23778475  0.83577906 -0.01366943  0.30375423 -0.44896874]\n",
      "new weights: [-0.26595599  0.34064899 -1.09977125 -0.49533485 -0.80648822 -0.91532281\n",
      " -0.72747958 -0.40887855 -0.30646505 -0.02236653 -0.26161097  0.270439\n",
      " -0.6910955   0.65623487 -1.04522481  0.24093502 -0.2653904   0.01737966\n",
      " -0.81922612 -0.70379702  0.50148914  0.83652315 -0.47315164  0.28464523\n",
      "  0.6527783   0.68921333 -0.92991158 -1.02189043 -0.76033916  0.65628501\n",
      " -0.90330633 -0.25778475  0.81577906 -0.03366943  0.28375423 -0.46896874]\n",
      "prediction: 1\n",
      "old weights: [-0.26595599  0.34064899 -1.09977125 -0.49533485 -0.80648822 -0.91532281\n",
      " -0.72747958 -0.40887855 -0.30646505 -0.02236653 -0.26161097  0.270439\n",
      " -0.6910955   0.65623487 -1.04522481  0.24093502 -0.2653904   0.01737966\n",
      " -0.81922612 -0.70379702  0.50148914  0.83652315 -0.47315164  0.28464523\n",
      "  0.6527783   0.68921333 -0.92991158 -1.02189043 -0.76033916  0.65628501\n",
      " -0.90330633 -0.25778475  0.81577906 -0.03366943  0.28375423 -0.46896874]\n",
      "new weights: [-0.28595599  0.32064899 -1.11977125 -0.51533485 -0.82648822 -0.93532281\n",
      " -0.74747958 -0.42887855 -0.32646505 -0.04236653 -0.28161097  0.250439\n",
      " -0.7110955   0.63623487 -1.06522481  0.22093502 -0.2853904  -0.00262034\n",
      " -0.83922612 -0.72379702  0.48148914  0.81652315 -0.49315164  0.26464523\n",
      "  0.6327783   0.66921333 -0.94991158 -1.04189043 -0.78033916  0.63628501\n",
      " -0.92330633 -0.27778475  0.79577906 -0.05366943  0.26375423 -0.48896874]\n",
      "prediction: 1\n",
      "old weights: [-0.28595599  0.32064899 -1.11977125 -0.51533485 -0.82648822 -0.93532281\n",
      " -0.74747958 -0.42887855 -0.32646505 -0.04236653 -0.28161097  0.250439\n",
      " -0.7110955   0.63623487 -1.06522481  0.22093502 -0.2853904  -0.00262034\n",
      " -0.83922612 -0.72379702  0.48148914  0.81652315 -0.49315164  0.26464523\n",
      "  0.6327783   0.66921333 -0.94991158 -1.04189043 -0.78033916  0.63628501\n",
      " -0.92330633 -0.27778475  0.79577906 -0.05366943  0.26375423 -0.48896874]\n",
      "new weights: [-0.30595599  0.30064899 -1.13977125 -0.53533485 -0.84648822 -0.95532281\n",
      " -0.76747958 -0.44887855 -0.34646505 -0.06236653 -0.30161097  0.230439\n",
      " -0.7310955   0.61623487 -1.08522481  0.20093502 -0.3053904  -0.02262034\n",
      " -0.85922612 -0.74379702  0.46148914  0.79652315 -0.51315164  0.24464523\n",
      "  0.6127783   0.64921333 -0.96991158 -1.06189043 -0.80033916  0.61628501\n",
      " -0.94330633 -0.29778475  0.77577906 -0.07366943  0.24375423 -0.50896874]\n",
      "prediction: 1\n",
      "old weights: [-0.30595599  0.30064899 -1.13977125 -0.53533485 -0.84648822 -0.95532281\n",
      " -0.76747958 -0.44887855 -0.34646505 -0.06236653 -0.30161097  0.230439\n",
      " -0.7310955   0.61623487 -1.08522481  0.20093502 -0.3053904  -0.02262034\n",
      " -0.85922612 -0.74379702  0.46148914  0.79652315 -0.51315164  0.24464523\n",
      "  0.6127783   0.64921333 -0.96991158 -1.06189043 -0.80033916  0.61628501\n",
      " -0.94330633 -0.29778475  0.77577906 -0.07366943  0.24375423 -0.50896874]\n",
      "new weights: [-0.32595599  0.28064899 -1.15977125 -0.55533485 -0.86648822 -0.97532281\n",
      " -0.78747958 -0.46887855 -0.36646505 -0.08236653 -0.32161097  0.210439\n",
      " -0.7510955   0.59623487 -1.10522481  0.18093502 -0.3253904  -0.04262034\n",
      " -0.87922612 -0.76379702  0.44148914  0.77652315 -0.53315164  0.22464523\n",
      "  0.5927783   0.62921333 -0.98991158 -1.08189043 -0.82033916  0.59628501\n",
      " -0.96330633 -0.31778475  0.75577906 -0.09366943  0.22375423 -0.52896874]\n",
      "prediction: 1\n",
      "old weights: [-0.32595599  0.28064899 -1.15977125 -0.55533485 -0.86648822 -0.97532281\n",
      " -0.78747958 -0.46887855 -0.36646505 -0.08236653 -0.32161097  0.210439\n",
      " -0.7510955   0.59623487 -1.10522481  0.18093502 -0.3253904  -0.04262034\n",
      " -0.87922612 -0.76379702  0.44148914  0.77652315 -0.53315164  0.22464523\n",
      "  0.5927783   0.62921333 -0.98991158 -1.08189043 -0.82033916  0.59628501\n",
      " -0.96330633 -0.31778475  0.75577906 -0.09366943  0.22375423 -0.52896874]\n",
      "new weights: [-0.34595599  0.26064899 -1.17977125 -0.57533485 -0.88648822 -0.99532281\n",
      " -0.80747958 -0.48887855 -0.38646505 -0.10236653 -0.34161097  0.190439\n",
      " -0.7710955   0.57623487 -1.12522481  0.16093502 -0.3453904  -0.06262034\n",
      " -0.89922612 -0.78379702  0.42148914  0.75652315 -0.55315164  0.20464523\n",
      "  0.5727783   0.60921333 -1.00991158 -1.10189043 -0.84033916  0.57628501\n",
      " -0.98330633 -0.33778475  0.73577906 -0.11366943  0.20375423 -0.54896874]\n",
      "prediction: 1\n",
      "old weights: [-0.34595599  0.26064899 -1.17977125 -0.57533485 -0.88648822 -0.99532281\n",
      " -0.80747958 -0.48887855 -0.38646505 -0.10236653 -0.34161097  0.190439\n",
      " -0.7710955   0.57623487 -1.12522481  0.16093502 -0.3453904  -0.06262034\n",
      " -0.89922612 -0.78379702  0.42148914  0.75652315 -0.55315164  0.20464523\n",
      "  0.5727783   0.60921333 -1.00991158 -1.10189043 -0.84033916  0.57628501\n",
      " -0.98330633 -0.33778475  0.73577906 -0.11366943  0.20375423 -0.54896874]\n",
      "new weights: [-0.36595599  0.24064899 -1.19977125 -0.59533485 -0.90648822 -1.01532281\n",
      " -0.82747958 -0.50887855 -0.40646505 -0.12236653 -0.36161097  0.170439\n",
      " -0.7910955   0.55623487 -1.14522481  0.14093502 -0.3653904  -0.08262034\n",
      " -0.91922612 -0.80379702  0.40148914  0.73652315 -0.57315164  0.18464523\n",
      "  0.5527783   0.58921333 -1.02991158 -1.12189043 -0.86033916  0.55628501\n",
      " -1.00330633 -0.35778475  0.71577906 -0.13366943  0.18375423 -0.56896874]\n",
      "prediction: 1\n",
      "old weights: [-0.36595599  0.24064899 -1.19977125 -0.59533485 -0.90648822 -1.01532281\n",
      " -0.82747958 -0.50887855 -0.40646505 -0.12236653 -0.36161097  0.170439\n",
      " -0.7910955   0.55623487 -1.14522481  0.14093502 -0.3653904  -0.08262034\n",
      " -0.91922612 -0.80379702  0.40148914  0.73652315 -0.57315164  0.18464523\n",
      "  0.5527783   0.58921333 -1.02991158 -1.12189043 -0.86033916  0.55628501\n",
      " -1.00330633 -0.35778475  0.71577906 -0.13366943  0.18375423 -0.56896874]\n",
      "new weights: [-0.38595599  0.22064899 -1.21977125 -0.61533485 -0.92648822 -1.03532281\n",
      " -0.84747958 -0.52887855 -0.42646505 -0.14236653 -0.38161097  0.150439\n",
      " -0.8110955   0.53623487 -1.16522481  0.12093502 -0.3853904  -0.10262034\n",
      " -0.93922612 -0.82379702  0.38148914  0.71652315 -0.59315164  0.16464523\n",
      "  0.5327783   0.56921333 -1.04991158 -1.14189043 -0.88033916  0.53628501\n",
      " -1.02330633 -0.37778475  0.69577906 -0.15366943  0.16375423 -0.58896874]\n",
      "prediction: 1\n",
      "old weights: [-0.38595599  0.22064899 -1.21977125 -0.61533485 -0.92648822 -1.03532281\n",
      " -0.84747958 -0.52887855 -0.42646505 -0.14236653 -0.38161097  0.150439\n",
      " -0.8110955   0.53623487 -1.16522481  0.12093502 -0.3853904  -0.10262034\n",
      " -0.93922612 -0.82379702  0.38148914  0.71652315 -0.59315164  0.16464523\n",
      "  0.5327783   0.56921333 -1.04991158 -1.14189043 -0.88033916  0.53628501\n",
      " -1.02330633 -0.37778475  0.69577906 -0.15366943  0.16375423 -0.58896874]\n",
      "new weights: [-0.40595599  0.20064899 -1.23977125 -0.63533485 -0.94648822 -1.05532281\n",
      " -0.86747958 -0.54887855 -0.44646505 -0.16236653 -0.40161097  0.130439\n",
      " -0.8310955   0.51623487 -1.18522481  0.10093502 -0.4053904  -0.12262034\n",
      " -0.95922612 -0.84379702  0.36148914  0.69652315 -0.61315164  0.14464523\n",
      "  0.5127783   0.54921333 -1.06991158 -1.16189043 -0.90033916  0.51628501\n",
      " -1.04330633 -0.39778475  0.67577906 -0.17366943  0.14375423 -0.60896874]\n",
      "prediction: 1\n",
      "old weights: [-0.40595599  0.20064899 -1.23977125 -0.63533485 -0.94648822 -1.05532281\n",
      " -0.86747958 -0.54887855 -0.44646505 -0.16236653 -0.40161097  0.130439\n",
      " -0.8310955   0.51623487 -1.18522481  0.10093502 -0.4053904  -0.12262034\n",
      " -0.95922612 -0.84379702  0.36148914  0.69652315 -0.61315164  0.14464523\n",
      "  0.5127783   0.54921333 -1.06991158 -1.16189043 -0.90033916  0.51628501\n",
      " -1.04330633 -0.39778475  0.67577906 -0.17366943  0.14375423 -0.60896874]\n",
      "new weights: [-0.42595599  0.18064899 -1.25977125 -0.65533485 -0.96648822 -1.07532281\n",
      " -0.88747958 -0.56887855 -0.46646505 -0.18236653 -0.42161097  0.110439\n",
      " -0.8510955   0.49623487 -1.20522481  0.08093502 -0.4253904  -0.14262034\n",
      " -0.97922612 -0.86379702  0.34148914  0.67652315 -0.63315164  0.12464523\n",
      "  0.4927783   0.52921333 -1.08991158 -1.18189043 -0.92033916  0.49628501\n",
      " -1.06330633 -0.41778475  0.65577906 -0.19366943  0.12375423 -0.62896874]\n",
      "prediction: 1\n",
      "old weights: [-0.42595599  0.18064899 -1.25977125 -0.65533485 -0.96648822 -1.07532281\n",
      " -0.88747958 -0.56887855 -0.46646505 -0.18236653 -0.42161097  0.110439\n",
      " -0.8510955   0.49623487 -1.20522481  0.08093502 -0.4253904  -0.14262034\n",
      " -0.97922612 -0.86379702  0.34148914  0.67652315 -0.63315164  0.12464523\n",
      "  0.4927783   0.52921333 -1.08991158 -1.18189043 -0.92033916  0.49628501\n",
      " -1.06330633 -0.41778475  0.65577906 -0.19366943  0.12375423 -0.62896874]\n",
      "new weights: [-0.44595599  0.16064899 -1.27977125 -0.67533485 -0.98648822 -1.09532281\n",
      " -0.90747958 -0.58887855 -0.48646505 -0.20236653 -0.44161097  0.090439\n",
      " -0.8710955   0.47623487 -1.22522481  0.06093502 -0.4453904  -0.16262034\n",
      " -0.99922612 -0.88379702  0.32148914  0.65652315 -0.65315164  0.10464523\n",
      "  0.4727783   0.50921333 -1.10991158 -1.20189043 -0.94033916  0.47628501\n",
      " -1.08330633 -0.43778475  0.63577906 -0.21366943  0.10375423 -0.64896874]\n",
      "prediction: 1\n",
      "old weights: [-0.44595599  0.16064899 -1.27977125 -0.67533485 -0.98648822 -1.09532281\n",
      " -0.90747958 -0.58887855 -0.48646505 -0.20236653 -0.44161097  0.090439\n",
      " -0.8710955   0.47623487 -1.22522481  0.06093502 -0.4453904  -0.16262034\n",
      " -0.99922612 -0.88379702  0.32148914  0.65652315 -0.65315164  0.10464523\n",
      "  0.4727783   0.50921333 -1.10991158 -1.20189043 -0.94033916  0.47628501\n",
      " -1.08330633 -0.43778475  0.63577906 -0.21366943  0.10375423 -0.64896874]\n",
      "new weights: [-0.46595599  0.14064899 -1.29977125 -0.69533485 -1.00648822 -1.11532281\n",
      " -0.92747958 -0.60887855 -0.50646505 -0.22236653 -0.46161097  0.070439\n",
      " -0.8910955   0.45623487 -1.24522481  0.04093502 -0.4653904  -0.18262034\n",
      " -1.01922612 -0.90379702  0.30148914  0.63652315 -0.67315164  0.08464523\n",
      "  0.4527783   0.48921333 -1.12991158 -1.22189043 -0.96033916  0.45628501\n",
      " -1.10330633 -0.45778475  0.61577906 -0.23366943  0.08375423 -0.66896874]\n",
      "prediction: 1\n",
      "old weights: [-0.46595599  0.14064899 -1.29977125 -0.69533485 -1.00648822 -1.11532281\n",
      " -0.92747958 -0.60887855 -0.50646505 -0.22236653 -0.46161097  0.070439\n",
      " -0.8910955   0.45623487 -1.24522481  0.04093502 -0.4653904  -0.18262034\n",
      " -1.01922612 -0.90379702  0.30148914  0.63652315 -0.67315164  0.08464523\n",
      "  0.4527783   0.48921333 -1.12991158 -1.22189043 -0.96033916  0.45628501\n",
      " -1.10330633 -0.45778475  0.61577906 -0.23366943  0.08375423 -0.66896874]\n",
      "new weights: [-0.48595599  0.12064899 -1.31977125 -0.71533485 -1.02648822 -1.13532281\n",
      " -0.94747958 -0.62887855 -0.52646505 -0.24236653 -0.48161097  0.050439\n",
      " -0.9110955   0.43623487 -1.26522481  0.02093502 -0.4853904  -0.20262034\n",
      " -1.03922612 -0.92379702  0.28148914  0.61652315 -0.69315164  0.06464523\n",
      "  0.4327783   0.46921333 -1.14991158 -1.24189043 -0.98033916  0.43628501\n",
      " -1.12330633 -0.47778475  0.59577906 -0.25366943  0.06375423 -0.68896874]\n",
      "prediction: 1\n",
      "old weights: [-0.48595599  0.12064899 -1.31977125 -0.71533485 -1.02648822 -1.13532281\n",
      " -0.94747958 -0.62887855 -0.52646505 -0.24236653 -0.48161097  0.050439\n",
      " -0.9110955   0.43623487 -1.26522481  0.02093502 -0.4853904  -0.20262034\n",
      " -1.03922612 -0.92379702  0.28148914  0.61652315 -0.69315164  0.06464523\n",
      "  0.4327783   0.46921333 -1.14991158 -1.24189043 -0.98033916  0.43628501\n",
      " -1.12330633 -0.47778475  0.59577906 -0.25366943  0.06375423 -0.68896874]\n",
      "new weights: [-5.05955991e-01  1.00648987e-01 -1.33977125e+00 -7.35334855e-01\n",
      " -1.04648822e+00 -1.15532281e+00 -9.67479577e-01 -6.48878546e-01\n",
      " -5.46465052e-01 -2.62366532e-01 -5.01610971e-01  3.04390008e-02\n",
      " -9.31095501e-01  4.16234873e-01 -1.28522481e+00  9.35020357e-04\n",
      " -5.05390395e-01 -2.22620343e-01 -1.05922612e+00 -9.43797022e-01\n",
      "  2.61489137e-01  5.96523151e-01 -7.13151644e-01  4.46452313e-02\n",
      "  4.12778305e-01  4.49213327e-01 -1.16991158e+00 -1.26189043e+00\n",
      " -1.00033916e+00  4.16285007e-01 -1.14330633e+00 -4.97784750e-01\n",
      "  5.75779060e-01 -2.73669430e-01  4.37542279e-02 -7.08968738e-01]\n",
      "prediction: 0\n",
      "old weights: [-5.05955991e-01  1.00648987e-01 -1.33977125e+00 -7.35334855e-01\n",
      " -1.04648822e+00 -1.15532281e+00 -9.67479577e-01 -6.48878546e-01\n",
      " -5.46465052e-01 -2.62366532e-01 -5.01610971e-01  3.04390008e-02\n",
      " -9.31095501e-01  4.16234873e-01 -1.28522481e+00  9.35020357e-04\n",
      " -5.05390395e-01 -2.22620343e-01 -1.05922612e+00 -9.43797022e-01\n",
      "  2.61489137e-01  5.96523151e-01 -7.13151644e-01  4.46452313e-02\n",
      "  4.12778305e-01  4.49213327e-01 -1.16991158e+00 -1.26189043e+00\n",
      " -1.00033916e+00  4.16285007e-01 -1.14330633e+00 -4.97784750e-01\n",
      "  5.75779060e-01 -2.73669430e-01  4.37542279e-02 -7.08968738e-01]\n",
      "new weights: [-0.51595599  0.09064899 -1.34977125 -0.74533485 -1.05648822 -1.16532281\n",
      " -0.97747958 -0.65887855 -0.55646505 -0.27236653 -0.51161097  0.020439\n",
      " -0.9410955   0.40623487 -1.29522481 -0.00906498 -0.5153904  -0.23262034\n",
      " -1.06922612 -0.95379702  0.25148914  0.58652315 -0.72315164  0.03464523\n",
      "  0.4027783   0.43921333 -1.17991158 -1.27189043 -1.01033916  0.40628501\n",
      " -1.15330633 -0.50778475  0.56577906 -0.28366943  0.03375423 -0.71896874]\n",
      "prediction: 0\n",
      "old weights: [-0.51595599  0.09064899 -1.34977125 -0.74533485 -1.05648822 -1.16532281\n",
      " -0.97747958 -0.65887855 -0.55646505 -0.27236653 -0.51161097  0.020439\n",
      " -0.9410955   0.40623487 -1.29522481 -0.00906498 -0.5153904  -0.23262034\n",
      " -1.06922612 -0.95379702  0.25148914  0.58652315 -0.72315164  0.03464523\n",
      "  0.4027783   0.43921333 -1.17991158 -1.27189043 -1.01033916  0.40628501\n",
      " -1.15330633 -0.50778475  0.56577906 -0.28366943  0.03375423 -0.71896874]\n",
      "new weights: [-0.52595599  0.08064899 -1.35977125 -0.75533485 -1.06648822 -1.17532281\n",
      " -0.98747958 -0.66887855 -0.56646505 -0.28236653 -0.52161097  0.010439\n",
      " -0.9510955   0.39623487 -1.30522481 -0.01906498 -0.5253904  -0.24262034\n",
      " -1.07922612 -0.96379702  0.24148914  0.57652315 -0.73315164  0.02464523\n",
      "  0.3927783   0.42921333 -1.18991158 -1.28189043 -1.02033916  0.39628501\n",
      " -1.16330633 -0.51778475  0.55577906 -0.29366943  0.02375423 -0.72896874]\n",
      "prediction: 0\n",
      "old weights: [-0.52595599  0.08064899 -1.35977125 -0.75533485 -1.06648822 -1.17532281\n",
      " -0.98747958 -0.66887855 -0.56646505 -0.28236653 -0.52161097  0.010439\n",
      " -0.9510955   0.39623487 -1.30522481 -0.01906498 -0.5253904  -0.24262034\n",
      " -1.07922612 -0.96379702  0.24148914  0.57652315 -0.73315164  0.02464523\n",
      "  0.3927783   0.42921333 -1.18991158 -1.28189043 -1.02033916  0.39628501\n",
      " -1.16330633 -0.51778475  0.55577906 -0.29366943  0.02375423 -0.72896874]\n",
      "new weights: [-5.35955991e-01  7.06489869e-02 -1.36977125e+00 -7.65334855e-01\n",
      " -1.07648822e+00 -1.18532281e+00 -9.97479577e-01 -6.78878546e-01\n",
      " -5.76465052e-01 -2.92366532e-01 -5.31610971e-01  4.39000794e-04\n",
      " -9.61095501e-01  3.86234873e-01 -1.31522481e+00 -2.90649796e-02\n",
      " -5.35390395e-01 -2.52620343e-01 -1.08922612e+00 -9.73797022e-01\n",
      "  2.31489137e-01  5.66523151e-01 -7.43151644e-01  1.46452313e-02\n",
      "  3.82778305e-01  4.19213327e-01 -1.19991158e+00 -1.29189043e+00\n",
      " -1.03033916e+00  3.86285007e-01 -1.17330633e+00 -5.27784750e-01\n",
      "  5.45779060e-01 -3.03669430e-01  1.37542279e-02 -7.38968738e-01]\n",
      "prediction: 0\n",
      "old weights: [-5.35955991e-01  7.06489869e-02 -1.36977125e+00 -7.65334855e-01\n",
      " -1.07648822e+00 -1.18532281e+00 -9.97479577e-01 -6.78878546e-01\n",
      " -5.76465052e-01 -2.92366532e-01 -5.31610971e-01  4.39000794e-04\n",
      " -9.61095501e-01  3.86234873e-01 -1.31522481e+00 -2.90649796e-02\n",
      " -5.35390395e-01 -2.52620343e-01 -1.08922612e+00 -9.73797022e-01\n",
      "  2.31489137e-01  5.66523151e-01 -7.43151644e-01  1.46452313e-02\n",
      "  3.82778305e-01  4.19213327e-01 -1.19991158e+00 -1.29189043e+00\n",
      " -1.03033916e+00  3.86285007e-01 -1.17330633e+00 -5.27784750e-01\n",
      "  5.45779060e-01 -3.03669430e-01  1.37542279e-02 -7.38968738e-01]\n",
      "new weights: [-0.54595599  0.06064899 -1.37977125 -0.77533485 -1.08648822 -1.19532281\n",
      " -1.00747958 -0.68887855 -0.58646505 -0.30236653 -0.54161097 -0.009561\n",
      " -0.9710955   0.37623487 -1.32522481 -0.03906498 -0.5453904  -0.26262034\n",
      " -1.09922612 -0.98379702  0.22148914  0.55652315 -0.75315164  0.00464523\n",
      "  0.3727783   0.40921333 -1.20991158 -1.30189043 -1.04033916  0.37628501\n",
      " -1.18330633 -0.53778475  0.53577906 -0.31366943  0.00375423 -0.74896874]\n",
      "prediction: 0\n",
      "old weights: [-0.54595599  0.06064899 -1.37977125 -0.77533485 -1.08648822 -1.19532281\n",
      " -1.00747958 -0.68887855 -0.58646505 -0.30236653 -0.54161097 -0.009561\n",
      " -0.9710955   0.37623487 -1.32522481 -0.03906498 -0.5453904  -0.26262034\n",
      " -1.09922612 -0.98379702  0.22148914  0.55652315 -0.75315164  0.00464523\n",
      "  0.3727783   0.40921333 -1.20991158 -1.30189043 -1.04033916  0.37628501\n",
      " -1.18330633 -0.53778475  0.53577906 -0.31366943  0.00375423 -0.74896874]\n",
      "new weights: [-0.55595599  0.05064899 -1.38977125 -0.78533485 -1.09648822 -1.20532281\n",
      " -1.01747958 -0.69887855 -0.59646505 -0.31236653 -0.55161097 -0.019561\n",
      " -0.9810955   0.36623487 -1.33522481 -0.04906498 -0.5553904  -0.27262034\n",
      " -1.10922612 -0.99379702  0.21148914  0.54652315 -0.76315164 -0.00535477\n",
      "  0.3627783   0.39921333 -1.21991158 -1.31189043 -1.05033916  0.36628501\n",
      " -1.19330633 -0.54778475  0.52577906 -0.32366943 -0.00624577 -0.75896874]\n",
      "prediction: 0\n",
      "old weights: [-0.55595599  0.05064899 -1.38977125 -0.78533485 -1.09648822 -1.20532281\n",
      " -1.01747958 -0.69887855 -0.59646505 -0.31236653 -0.55161097 -0.019561\n",
      " -0.9810955   0.36623487 -1.33522481 -0.04906498 -0.5553904  -0.27262034\n",
      " -1.10922612 -0.99379702  0.21148914  0.54652315 -0.76315164 -0.00535477\n",
      "  0.3627783   0.39921333 -1.21991158 -1.31189043 -1.05033916  0.36628501\n",
      " -1.19330633 -0.54778475  0.52577906 -0.32366943 -0.00624577 -0.75896874]\n",
      "new weights: [-0.56595599  0.04064899 -1.39977125 -0.79533485 -1.10648822 -1.21532281\n",
      " -1.02747958 -0.70887855 -0.60646505 -0.32236653 -0.56161097 -0.029561\n",
      " -0.9910955   0.35623487 -1.34522481 -0.05906498 -0.5653904  -0.28262034\n",
      " -1.11922612 -1.00379702  0.20148914  0.53652315 -0.77315164 -0.01535477\n",
      "  0.3527783   0.38921333 -1.22991158 -1.32189043 -1.06033916  0.35628501\n",
      " -1.20330633 -0.55778475  0.51577906 -0.33366943 -0.01624577 -0.76896874]\n",
      "prediction: 0\n",
      "old weights: [-0.56595599  0.04064899 -1.39977125 -0.79533485 -1.10648822 -1.21532281\n",
      " -1.02747958 -0.70887855 -0.60646505 -0.32236653 -0.56161097 -0.029561\n",
      " -0.9910955   0.35623487 -1.34522481 -0.05906498 -0.5653904  -0.28262034\n",
      " -1.11922612 -1.00379702  0.20148914  0.53652315 -0.77315164 -0.01535477\n",
      "  0.3527783   0.38921333 -1.22991158 -1.32189043 -1.06033916  0.35628501\n",
      " -1.20330633 -0.55778475  0.51577906 -0.33366943 -0.01624577 -0.76896874]\n",
      "new weights: [-0.57595599  0.03064899 -1.40977125 -0.80533485 -1.11648822 -1.22532281\n",
      " -1.03747958 -0.71887855 -0.61646505 -0.33236653 -0.57161097 -0.039561\n",
      " -1.0010955   0.34623487 -1.35522481 -0.06906498 -0.5753904  -0.29262034\n",
      " -1.12922612 -1.01379702  0.19148914  0.52652315 -0.78315164 -0.02535477\n",
      "  0.3427783   0.37921333 -1.23991158 -1.33189043 -1.07033916  0.34628501\n",
      " -1.21330633 -0.56778475  0.50577906 -0.34366943 -0.02624577 -0.77896874]\n",
      "prediction: 0\n",
      "old weights: [-0.57595599  0.03064899 -1.40977125 -0.80533485 -1.11648822 -1.22532281\n",
      " -1.03747958 -0.71887855 -0.61646505 -0.33236653 -0.57161097 -0.039561\n",
      " -1.0010955   0.34623487 -1.35522481 -0.06906498 -0.5753904  -0.29262034\n",
      " -1.12922612 -1.01379702  0.19148914  0.52652315 -0.78315164 -0.02535477\n",
      "  0.3427783   0.37921333 -1.23991158 -1.33189043 -1.07033916  0.34628501\n",
      " -1.21330633 -0.56778475  0.50577906 -0.34366943 -0.02624577 -0.77896874]\n",
      "new weights: [-0.58595599  0.02064899 -1.41977125 -0.81533485 -1.12648822 -1.23532281\n",
      " -1.04747958 -0.72887855 -0.62646505 -0.34236653 -0.58161097 -0.049561\n",
      " -1.0110955   0.33623487 -1.36522481 -0.07906498 -0.5853904  -0.30262034\n",
      " -1.13922612 -1.02379702  0.18148914  0.51652315 -0.79315164 -0.03535477\n",
      "  0.3327783   0.36921333 -1.24991158 -1.34189043 -1.08033916  0.33628501\n",
      " -1.22330633 -0.57778475  0.49577906 -0.35366943 -0.03624577 -0.78896874]\n"
     ]
    }
   ],
   "source": [
    "# Setup labels vector for recognizing 'A'\n",
    "labels = [-1] * len(train_set)\n",
    "labels[0] = 1\n",
    "\n",
    "# Add +1 bias to beginning of all input vectors in training set\n",
    "inputs = [[1] + train_set[i] for i in range(len(train_set))]\n",
    "\n",
    "# randomize weights\n",
    "np.random.seed(1)\n",
    "weights = np.random.uniform(-1, 1, (len(inputs[0])))\n",
    "\n",
    "# load in testing set\n",
    "test_inputs = [[1] + test_set[i] for i in range(len(test_set))]\n",
    "\n",
    "# convert to np.arrays\n",
    "labels = np.asarray(labels)\n",
    "inputs = np.asarray(inputs)\n",
    "weights = np.asarray(weights)\n",
    "test_inputs = np.asarray(test_inputs)\n",
    "\n",
    "def step(weights, inputs, threshold=0):\n",
    "    \"\"\" A simple step function \"\"\"\n",
    "    summation = np.dot(weights, inputs)\n",
    "    return 1 if summation > threshold else 0\n",
    "\n",
    "def predict(weights, inputs, activation=step):\n",
    "    \"\"\" Predicts a classification for the given weights and inputs. \"\"\"\n",
    "    return activation(weights, inputs)\n",
    "\n",
    "def update_weight(weight, inp, prediction, label, learning_rate=0.01):\n",
    "    return weight + (learning_rate * (label - prediction))  # w + delta_w\n",
    "\n",
    "def update_weights(weights, inputs, prediction, label, learning_rate=0.01):\n",
    "    return np.asarray([update_weight(w, inputs, prediction, label) for w in weights])\n",
    "\n",
    "predictions = [predict(weights, inputs[i]) for i in range(len(inputs))]\n",
    "print(weights)\n",
    "#print(update_weights(weights, inputs[0], predictions[0], labels[0]))\n",
    "\n",
    "\n",
    "# test all inputs, return number of missclassifications\n",
    "def accuracy():\n",
    "\n",
    "def train(weights, inputs, labels):\n",
    "    for inp, labl in zip(inputs, labels):\n",
    "        prediction = predict(weights, inputs)\n",
    "        #print(f'prediction: {prediction}')\n",
    "        #print(f'old weights: {weights}')\n",
    "        if prediction\n",
    "        weights = update_weights(weights, inp, prediction, labl)\n",
    "        #print(f'new weights: {weights}')\n",
    "\n",
    "train(weights, inputs[0], labels)\n",
    "#char_map = [ch for ch in \"abcdefghijklmnopqrstuvwxyz\"]\n",
    "#all_labels = [np.roll(labels, i) for i in range(len(labels))]  # generate labels for all letters we want to classify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
